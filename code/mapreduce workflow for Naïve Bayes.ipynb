{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUUSQI9xiqVf"
      },
      "outputs": [],
      "source": [
        "# --- Imports & session -------------------------------------------------------\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import math, time, os, shutil, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# --- Session Spark -----------------------------------------------------------\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"NaiveBayes_MapReduce_like\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Réduit le bruit des logs Spark pour ne garder que les avertissements et erreurs.\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# --- Hyperparam / config par défaut -----------------------------------------\n",
        "ALPHA = 1.0       # Lissage de Laplace appliqué aux proba conditionnelles\n",
        "seed = 7          # Graine pour la reproductibilité\n",
        "train_ratio = 0.7 # Protocole : 70% train / 30% test\n",
        "has_header = False\n",
        "sep = \",\"\n",
        "\n",
        "# --- Données & sorties intermédiaires ---------------------------------------\n",
        "data_path = \"poker-hand-training-true.data\"  # Jeu UCI Poker-Hand\n",
        "out_dir = \"/tmp/nb_counts\"                   # Dossier pour stocker les comptages « MapReduce-like »\n",
        "\n",
        "# Remise à zéro des sorties intermédiaires\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU8NGSveiq4j"
      },
      "outputs": [],
      "source": [
        "def prepare_columns(df, has_header: bool):\n",
        "    \"\"\"\n",
        "    Prépare un DataFrame pour l'entraînement :\n",
        "    - la DERNIÈRE colonne est la colonne cible (label),\n",
        "    - renomme proprement les colonnes si le fichier n'a pas d'en-tête,\n",
        "      afin d'obtenir: features = f0, f1, ..., f{n-1} et label = \"label\".\n",
        "\n",
        "    Paramètres\n",
        "    ----------\n",
        "    df : pyspark.sql.DataFrame\n",
        "        Données d'entrée (colonnes ordonnées: features puis label en dernier).\n",
        "    has_header : bool\n",
        "        True si le fichier source contient un en-tête déjà exploitable.\n",
        "\n",
        "    Retour\n",
        "    ------\n",
        "    (df, feature_cols, label_col) : tuple\n",
        "        - df : DataFrame éventuellement renommé,\n",
        "        - feature_cols : liste des noms de colonnes de caractéristiques,\n",
        "        - label_col : nom de la colonne cible.\n",
        "    \"\"\"\n",
        "    cols = df.columns  # ordre des colonnes tel que lu\n",
        "\n",
        "    if has_header:\n",
        "        # Cas \"avec en-tête\" : on garde les noms tels quels.\n",
        "        # Hypothèse: la dernière colonne du fichier est bien le label.\n",
        "        label_col = cols[-1]\n",
        "        feature_cols = cols[:-1]\n",
        "    else:\n",
        "        # Cas \"sans en-tête\" : la dernière colonne = label, le reste = features\n",
        "        label_col = cols[-1]\n",
        "        feature_cols = cols[:-1]\n",
        "\n",
        "        # Renommer les features en f0, f1, ..., et le label en \"label\"\n",
        "        # -> rend le pipeline plus simple et déterministe\n",
        "        ren = (\n",
        "            [F.col(c).alias(f\"f{i}\") for i, c in enumerate(feature_cols)]\n",
        "            + [F.col(label_col).alias(\"label\")]\n",
        "        )\n",
        "        df = df.select(*ren)\n",
        "\n",
        "        # Recrée les listes de noms après renommage\n",
        "        feature_cols = [f\"f{i}\" for i in range(len(feature_cols))]\n",
        "        label_col = \"label\"\n",
        "\n",
        "    return df, feature_cols, label_col\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKAuSyrpitKL",
        "outputId": "5dbb7859-cf93-4a6e-9a5d-03a31dda006a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+---+----+---+----+---+----+---+----+-----+\n",
            "| f0|  f1| f2|  f3| f4|  f5| f6|  f7| f8|  f9|label|\n",
            "+---+----+---+----+---+----+---+----+---+----+-----+\n",
            "|1.0|10.0|1.0|11.0|1.0|13.0|1.0|12.0|1.0| 1.0|  9.0|\n",
            "|2.0|11.0|2.0|13.0|2.0|10.0|2.0|12.0|2.0| 1.0|  9.0|\n",
            "|3.0|12.0|3.0|11.0|3.0|13.0|3.0|10.0|3.0| 1.0|  9.0|\n",
            "|4.0|10.0|4.0|11.0|4.0| 1.0|4.0|13.0|4.0|12.0|  9.0|\n",
            "|4.0| 1.0|4.0|13.0|4.0|12.0|4.0|11.0|4.0|10.0|  9.0|\n",
            "+---+----+---+----+---+----+---+----+---+----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Features: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\n",
            "Label   : label\n"
          ]
        }
      ],
      "source": [
        "# --- Lecture brute -----------------------------------------------------------\n",
        "raw = (\n",
        "    spark.read\n",
        "    .option(\"header\", has_header)   # True si le CSV a une ligne d’en-tête\n",
        "    .option(\"inferSchema\", True)    # Infère automatiquement les types\n",
        "    .option(\"sep\", sep)             # Séparateur de colonnes (\",\")\n",
        "    .csv(data_path)                 # Fichier source\n",
        ")\n",
        "\n",
        "# --- Préparation des noms de colonnes (et label en dernière position) -------\n",
        "df, feature_cols, label_col = prepare_columns(raw, has_header)\n",
        "\n",
        "# --- Mise au bon type pour Spark ML -----------------------------------------\n",
        "# Les algos/transformers Spark ML attendent des types numériques (DoubleType).\n",
        "# On caste donc toutes les colonnes (features + label) en double.\n",
        "for c in feature_cols + [label_col]:\n",
        "    df = df.withColumn(c, F.col(c).cast(\"double\"))\n",
        "\n",
        "# --- Cache pour accélérer les étapes suivantes ------------------------------\n",
        "# On met en cache car le DataFrame sera réutilisé plusieurs fois (splits, comptages, etc.).\n",
        "df.cache()\n",
        "\n",
        "# On vérifie visuellement le schéma/échantillon\n",
        "df.show(5)\n",
        "\n",
        "# Traçabilité : liste des features et nom du label\n",
        "print(\"Features:\", feature_cols)\n",
        "print(\"Label   :\", label_col)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfh1fsg7ivDl",
        "outputId": "7746653d-7e22-4544-ca39-7b69b2b66b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train = 17483  / Test = 7527\n"
          ]
        }
      ],
      "source": [
        "# --- Split entraînement / test ----------------------------------------------\n",
        "# randomSplit sépare aléatoirement selon des poids (approx.)\n",
        "train_df, test_df = df.randomSplit([train_ratio, 1 - train_ratio], seed=seed)\n",
        "\n",
        "# Ces DataFrames seront réutilisés (comptages, entraînement, évaluation) :\n",
        "# on les met en cache pour éviter de relire/calculer plusieurs fois.\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "# count() force l’évaluation paresseuse (lazy) de Spark, matérialise le cache\n",
        "# et affiche les tailles effectives des splits.\n",
        "print(\"Train =\", train_df.count(), \" / Test =\", test_df.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725Z1DzX2crA",
        "outputId": "07ddd9e6-dba3-413f-c5b1-713f9b4b36e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7527"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Détection des colonnes de features f0..fN\n",
        "feature_cols = sorted(\n",
        "    [c for c in train_df.columns if c.startswith(\"f\")],\n",
        "    key=lambda x: int(x[1:]) if x[1:].isdigit() else 10**9\n",
        ")\n",
        "assert feature_cols, \"Aucune colonne 'f*' détectée dans train_df.\"\n",
        "\n",
        "# 2) Normalisation (Naïve Bayes discret)\n",
        "def normalize_df(df):\n",
        "    \"\"\"Cast features/label en string; renomme label -> 'y'.\"\"\"\n",
        "    cols = [F.col(c).cast(\"string\").alias(c) for c in feature_cols]\n",
        "    return df.select(*cols, F.col(\"label\").cast(\"string\").alias(\"y\"))\n",
        "\n",
        "# 3) Passage wide -> long\n",
        "def to_long(df, keep_cols):\n",
        "    \"\"\"Wide → long: une ligne par (pos, value); conserve keep_cols.\"\"\"\n",
        "    arr = F.array(*[F.col(c) for c in feature_cols])\n",
        "    return df.select(*keep_cols, F.posexplode(arr).alias(\"pos\", \"value\"))\n",
        "\n",
        "# 4) Redimensionnement du jeu d’entraînement (pour la scalabilité)\n",
        "def scale_dataset(df, factor):\n",
        "    \"\"\"Resize: <1 sous-échant.; =1 inchangé; >1 sur-échant. avec remise.\"\"\"\n",
        "    if factor == 1.0:\n",
        "        return df\n",
        "    return df.sample(withReplacement=(factor > 1.0), fraction=factor, seed=42)\n",
        "\n",
        "# 5) Datasets normalisés\n",
        "train_norm = normalize_df(train_df).cache(); train_norm.count()\n",
        "test_norm  = normalize_df(test_df ).cache(); test_norm.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9m2s6j0gXI"
      },
      "source": [
        "## RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr2YZWWjyg6s"
      },
      "outputs": [],
      "source": [
        "# === Naive Bayes RDD (discret) avec lissage de Laplace ALPHA=1.0 =============\n",
        "\n",
        "def rdd_train(train_df_norm):\n",
        "    \"\"\"Construit le modèle: logprior, logcond, valeur par défaut, classes, |V_i|.\"\"\"\n",
        "    # RDD de (y, [(pos, value), ...]) pour compter facilement par (i,v,c)\n",
        "    def row_to_pair(row):\n",
        "        feats = [(i, getattr(row, feature_cols[i])) for i in range(len(feature_cols))]\n",
        "        return (row['y'], feats)\n",
        "\n",
        "    rdd = train_df_norm.rdd.map(row_to_pair).cache()\n",
        "\n",
        "    # Priors N_c puis P(c) lissé\n",
        "    N_c = rdd.map(lambda yc: (yc[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "    N = N_c.map(lambda kv: kv[1]).sum()\n",
        "    classes = N_c.map(lambda kv: kv[0]).collect()\n",
        "    C = len(classes)\n",
        "    logprior = N_c.mapValues(lambda nc: math.log((nc + ALPHA) / (N + ALPHA * C))).collectAsMap()\n",
        "\n",
        "    # Comptes N_{i,v,c}\n",
        "    ivc = (\n",
        "        rdd.flatMap(lambda yc: [((i, v, yc[0]), 1) for (i, v) in yc[1]])\n",
        "           .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "\n",
        "    # Dénominateurs N_{i,*,c} pour chaque (i,c)\n",
        "    ic = (\n",
        "        ivc.map(lambda kv: ((kv[0][0], kv[0][2]), kv[1]))\n",
        "           .reduceByKey(lambda a, b: a + b)\n",
        "           .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # Tailles de vocabulaires |V_i|\n",
        "    Vi = (\n",
        "        rdd.flatMap(lambda yc: [((i, v), 1) for (i, v) in yc[1]])\n",
        "           .distinct()\n",
        "           .map(lambda kv: (kv[0][0], 1))\n",
        "           .reduceByKey(lambda a, b: a + b)\n",
        "           .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # log P(v|c,i) lissé\n",
        "    def to_logcond(rec):\n",
        "        (i, v, c), n = rec\n",
        "        denom = ic[(i, c)]\n",
        "        V = Vi[i]\n",
        "        return ((i, v, c), math.log((n + ALPHA) / (denom + ALPHA * V)))\n",
        "\n",
        "    logcond = ivc.map(to_logcond).collectAsMap()\n",
        "\n",
        "    # Valeur par défaut pour v non vu: log(α / (N_{i,*,c} + α|V_i|))\n",
        "    default_logcond = {\n",
        "        (i, c): math.log(ALPHA / (ic[(i, c)] + ALPHA * Vi[i]))\n",
        "        for (i, c) in ic.keys()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"logprior\": logprior,\n",
        "        \"logcond\": logcond,\n",
        "        \"default\": default_logcond,\n",
        "        \"classes\": classes,\n",
        "        \"Vi\": Vi,\n",
        "    }\n",
        "\n",
        "def rdd_predict(test_df_norm, model):\n",
        "    \"\"\"Retourne un RDD de (pred, y_true) en scorant en log-espace.\"\"\"\n",
        "    logprior = model[\"logprior\"]; logcond = model[\"logcond\"]; default = model[\"default\"]\n",
        "    classes = model[\"classes\"]\n",
        "\n",
        "    def score_row(row):\n",
        "        feats = [(i, getattr(row, feature_cols[i])) for i in range(len(feature_cols))]\n",
        "        best_c, best_s = None, -1e100\n",
        "        for c in classes:\n",
        "            s = logprior[c]\n",
        "            for (i, v) in feats:\n",
        "                s += logcond.get((i, v, c), default[(i, c)])\n",
        "            if s > best_s:\n",
        "                best_s, best_c = s, c\n",
        "        return (best_c, row['y'])\n",
        "\n",
        "    return test_df_norm.rdd.map(score_row)\n",
        "\n",
        "def rdd_run(train_df_norm, test_df_norm):\n",
        "    \"\"\"Entraîne, infère, renvoie (accuracy, train_time, infer_time).\"\"\"\n",
        "    t0 = time.time()\n",
        "    model = rdd_train(train_df_norm)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    t1 = time.time()\n",
        "    preds = rdd_predict(test_df_norm, model).cache()\n",
        "    _ = preds.count()  # force l'exécution pour mesurer l'inférence\n",
        "    infer_time = time.time() - t1\n",
        "\n",
        "    accuracy = preds.map(lambda pr: 1 if pr[0] == pr[1] else 0).mean()\n",
        "    return float(accuracy), train_time, infer_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Jqoxvx5K6s",
        "outputId": "0a96c820-2b84-45a9-b495-b84e21b85ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RDD NB] accuracy=0.4925 | train_time=4.057s | infer_time=0.811s\n"
          ]
        }
      ],
      "source": [
        "# Exécute le pipeline RDD NB et affiche les métriques principales\n",
        "acc, ttrain, tinfer = rdd_run(train_norm, test_norm)\n",
        "print(f\"[RDD NB] accuracy={acc:.4f} | train_time={ttrain:.3f}s | infer_time={tinfer:.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFw5D6Ig0eJu"
      },
      "source": [
        "## DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VMJdYpgyj-d"
      },
      "outputs": [],
      "source": [
        "# === Naive Bayes DataFrame (discret), Laplace ALPHA=1.0 ======================\n",
        "\n",
        "def df_train(train_df_norm):\n",
        "    \"\"\"Calcule prior/logprior, logcond et valeurs par défaut en format DF.\"\"\"\n",
        "    # Format long: une ligne par (pos, value) observé\n",
        "    train_long = to_long(train_df_norm, keep_cols=[\"y\"]).cache(); train_long.count()\n",
        "\n",
        "    # Priors P(c) lissés\n",
        "    prior = train_long.groupBy(\"y\").count().cache(); prior.count()\n",
        "    N = prior.agg(F.sum(\"count\").alias(\"N\")).first()[\"N\"]\n",
        "    C = prior.count()\n",
        "    prior_log = (\n",
        "        prior.withColumn(\n",
        "            \"logprior\",\n",
        "            F.log((F.col(\"count\") + F.lit(ALPHA)) / (F.lit(N) + F.lit(ALPHA * C)))\n",
        "        ).select(\"y\", \"logprior\")\n",
        "    )\n",
        "\n",
        "    # Comptes conditionnels\n",
        "    ivc = (\n",
        "        train_long.groupBy(\"pos\", \"value\", \"y\").count()\n",
        "                  .withColumnRenamed(\"count\", \"n_ivc\")\n",
        "                  .cache()\n",
        "    ); ivc.count()\n",
        "\n",
        "    ic = ivc.groupBy(\"pos\", \"y\").agg(F.sum(\"n_ivc\").alias(\"n_i_star_c\")).cache(); ic.count()\n",
        "    Vi = train_long.groupBy(\"pos\").agg(F.countDistinct(\"value\").alias(\"V\")).cache(); Vi.count()\n",
        "\n",
        "    # log P(v|c,i) lissé\n",
        "    cond = (\n",
        "        ivc.join(ic, [\"pos\", \"y\"])\n",
        "           .join(Vi, [\"pos\"])\n",
        "           .withColumn(\n",
        "               \"logcond\",\n",
        "               F.log((F.col(\"n_ivc\") + F.lit(ALPHA)) /\n",
        "                     (F.col(\"n_i_star_c\") + F.lit(ALPHA) * F.col(\"V\")))\n",
        "           )\n",
        "           .select(\"pos\", \"value\", \"y\", \"logcond\")\n",
        "           .cache()\n",
        "    ); cond.count()\n",
        "\n",
        "    # Valeur par défaut pour v non vu\n",
        "    default = (\n",
        "        ic.join(Vi, [\"pos\"])\n",
        "          .withColumn(\n",
        "              \"default_logcond\",\n",
        "              F.log(F.lit(ALPHA) / (F.col(\"n_i_star_c\") + F.lit(ALPHA) * F.col(\"V\")))\n",
        "          )\n",
        "          .select(\"pos\", \"y\", \"default_logcond\")\n",
        "          .cache()\n",
        "    ); default.count()\n",
        "\n",
        "    return {\"prior_log\": prior_log, \"cond\": cond, \"default\": default}\n",
        "\n",
        "\n",
        "def df_predict(test_df_norm, model):\n",
        "    \"\"\"Score en log-espace + argmax par id; retourne (accuracy, preds DF).\"\"\"\n",
        "    # Id unique + label vrai\n",
        "    test_rows = (\n",
        "        test_df_norm.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "                    .withColumnRenamed(\"y\", \"y_true\")\n",
        "                    .cache()\n",
        "    ); test_rows.count()\n",
        "\n",
        "    # Format long\n",
        "    test_long = to_long(test_rows, keep_cols=[\"id\", \"y_true\"]).cache(); test_long.count()\n",
        "\n",
        "    # Espace des classes\n",
        "    classes = model[\"prior_log\"].select(F.col(\"y\").alias(\"y\")).cache(); classes.count()\n",
        "\n",
        "    # Étendre (id,pos,value) à toutes les classes\n",
        "    base = (\n",
        "        test_long.select(\"id\", \"pos\", \"value\").dropDuplicates([\"id\", \"pos\", \"value\"])\n",
        "                 .crossJoin(classes)\n",
        "                 .cache()\n",
        "    ); base.count()\n",
        "\n",
        "    # Joins: logcond si observé, sinon default_logcond -> somme + prior\n",
        "    joined = (\n",
        "        base.join(model[\"cond\"], [\"pos\", \"value\", \"y\"], \"left\")\n",
        "            .join(model[\"default\"], [\"pos\", \"y\"], \"left\")\n",
        "            .withColumn(\"logc\", F.coalesce(F.col(\"logcond\"), F.col(\"default_logcond\")))\n",
        "            .groupBy(\"id\", \"y\").agg(F.sum(\"logc\").alias(\"sum_logc\"))\n",
        "            .join(model[\"prior_log\"], [\"y\"])\n",
        "            .withColumn(\"score\", F.col(\"sum_logc\") + F.col(\"logprior\"))\n",
        "            .cache()\n",
        "    ); joined.count()\n",
        "\n",
        "    # Argmax(y) par id\n",
        "    w = Window.partitionBy(\"id\").orderBy(F.col(\"score\").desc())\n",
        "    pred = (\n",
        "        joined.withColumn(\"rn\", F.row_number().over(w))\n",
        "              .where(\"rn = 1\")\n",
        "              .select(\"id\", F.col(\"y\").alias(\"pred\"))\n",
        "              .cache()\n",
        "    ); pred.count()\n",
        "\n",
        "    # Accuracy\n",
        "    acc = (\n",
        "        pred.join(test_rows.select(\"id\", \"y_true\"), \"id\")\n",
        "            .withColumn(\"ok\", (F.col(\"pred\") == F.col(\"y_true\")).cast(\"int\"))\n",
        "            .agg(F.avg(\"ok\")).first()[0]\n",
        "    )\n",
        "\n",
        "    return float(acc), pred\n",
        "\n",
        "\n",
        "def df_run(train_df_norm, test_df_norm):\n",
        "    \"\"\"Entraîne DF NB puis infère; renvoie (acc, train_time, infer_time).\"\"\"\n",
        "    t0 = time.time()\n",
        "    model = df_train(train_df_norm)\n",
        "    # Matérialiser le \"modèle\" DF\n",
        "    model[\"cond\"].count(); model[\"default\"].count(); model[\"prior_log\"].count()\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    t1 = time.time()\n",
        "    acc, pred = df_predict(test_df_norm, model)\n",
        "    pred.count()  # force l’exécution pour mesurer l’inférence\n",
        "    infer_time = time.time() - t1\n",
        "\n",
        "    return acc, train_time, infer_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEHcKv9e5Q-C",
        "outputId": "fd0dbf92-537f-4ddc-b040-4b62bd3268a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DataFrame NB] accuracy=0.4924 | train_time=9.464s | infer_time=16.036s\n"
          ]
        }
      ],
      "source": [
        "# Exécute le pipeline DF NB et affiche les métriques principales\n",
        "acc, ttrain, tinfer = df_run(train_norm, test_norm)\n",
        "print(f\"[DataFrame NB] accuracy={acc:.4f} | train_time={ttrain:.3f}s | infer_time={tinfer:.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiUQeW9kq0Y8"
      },
      "source": [
        "## Spark ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvitUkeOrwqZ",
        "outputId": "91314177-f770-418c-8af2-bd54cf928df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Spark ML NB] accuracy=0.5025 | train_time=1.956s | infer_time=0.928s\n"
          ]
        }
      ],
      "source": [
        "# 1) Vectoriser les f* en une colonne \"features\" --------------------------------\n",
        "# Récupère toutes les colonnes de features (f0, f1, ..., fN), triées par index numérique.\n",
        "feature_cols = sorted(\n",
        "    [c for c in train_df.columns if c.startswith(\"f\")],\n",
        "    key=lambda x: int(x[1:]) if x[1:].isdigit() else 10**9\n",
        ")\n",
        "\n",
        "# Assemble les colonnes f* en un unique vecteur dense \"features\".\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Applique l’assembleur aux splits et ne garde que (features, label).\n",
        "train_ml = assembler.transform(train_df).select(\"features\", \"label\")\n",
        "test_ml  = assembler.transform(test_df ).select(\"features\", \"label\")\n",
        "\n",
        "# 2) Entraîner et évaluer le NaiveBayes Spark ML\n",
        "t0 = time.time()\n",
        "nb = NaiveBayes(\n",
        "    modelType=\"multinomial\",  # Adapté aux features non négatives.\n",
        "    smoothing=1.0,            # Lissage de Laplace (α = 1).\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "model = nb.fit(train_ml)             # Entraînement\n",
        "train_time = time.time() - t0        # Temps d'entraînement (s)\n",
        "\n",
        "# Inférence sur le set de test\n",
        "t1 = time.time()\n",
        "pred = model.transform(test_ml).select(\"prediction\", \"label\").cache()\n",
        "pred.count()\n",
        "infer_time = time.time() - t1        # Temps d'inférence (s)\n",
        "\n",
        "# Accuracy\n",
        "acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ").evaluate(pred)\n",
        "\n",
        "print(f\"[Spark ML NB] accuracy={acc:.4f} | train_time={train_time:.3f}s | infer_time={infer_time:.3f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNgDPWex4ah"
      },
      "source": [
        "## Scalabilité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCD4nHvHyloc",
        "outputId": "2c4e4b9f-491b-4c73-e901-21d0cb2224ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------------+----------+-------+------+-------------------+------------------+------------------+\n",
            "|approach |train_fraction|partitions|n_train|n_test|accuracy           |train_time_s      |infer_time_s      |\n",
            "+---------+--------------+----------+-------+------+-------------------+------------------+------------------+\n",
            "|DataFrame|0.25          |2         |4474   |7527  |0.4810681546432842 |16.66138195991516 |26.374945402145386|\n",
            "|DataFrame|0.25          |4         |4474   |7527  |0.4810681546432842 |11.070682764053345|26.290027618408203|\n",
            "|DataFrame|0.25          |8         |4474   |7527  |0.4810681546432842 |10.7231125831604  |20.868717908859253|\n",
            "|DataFrame|0.5           |2         |8813   |7527  |0.48505380629732964|8.716960906982422 |15.216153860092163|\n",
            "|DataFrame|0.5           |4         |8813   |7527  |0.48505380629732964|5.920252084732056 |14.192898750305176|\n",
            "|DataFrame|0.5           |8         |8813   |7527  |0.48505380629732964|7.961805105209351 |14.933992862701416|\n",
            "|DataFrame|1.0           |2         |17483  |7527  |0.49236083432974626|4.93933892250061  |10.16253399848938 |\n",
            "|DataFrame|1.0           |4         |17483  |7527  |0.49236083432974626|5.240325450897217 |14.02519416809082 |\n",
            "|DataFrame|1.0           |8         |17483  |7527  |0.49236083432974626|9.005699634552002 |14.677996635437012|\n",
            "|DataFrame|2.0           |2         |34845  |7527  |0.489969443337319  |6.306809902191162 |12.84010648727417 |\n",
            "|DataFrame|2.0           |4         |34845  |7527  |0.489969443337319  |6.49958872795105  |12.327281475067139|\n",
            "|DataFrame|2.0           |8         |34845  |7527  |0.489969443337319  |7.967953205108643 |14.250906705856323|\n",
            "|RDD      |0.25          |2         |4474   |7527  |0.481201009698419  |9.372457027435303 |1.7299811840057373|\n",
            "|RDD      |0.25          |4         |4474   |7527  |0.4812010096984188 |11.343065977096558|1.823739767074585 |\n",
            "|RDD      |0.25          |8         |4474   |7527  |0.48120100969841906|17.853020429611206|2.7179901599884033|\n",
            "|RDD      |0.5           |2         |8813   |7527  |0.4850538062973302 |4.953805208206177 |1.21681809425354  |\n",
            "|RDD      |0.5           |4         |8813   |7527  |0.4850538062973294 |9.145080089569092 |1.2689189910888672|\n",
            "|RDD      |0.5           |8         |8813   |7527  |0.48505380629732964|13.058752298355103|2.2122581005096436|\n",
            "|RDD      |1.0           |2         |17483  |7527  |0.4924936893848816 |5.476104736328125 |1.0473666191101074|\n",
            "|RDD      |1.0           |4         |17483  |7527  |0.49249368938488136|6.630896806716919 |2.1827995777130127|\n",
            "|RDD      |1.0           |8         |17483  |7527  |0.4924936893848812 |13.354767799377441|2.09151554107666  |\n",
            "|RDD      |2.0           |2         |34845  |7527  |0.4901022983924535 |6.375981092453003 |2.0391998291015625|\n",
            "|RDD      |2.0           |4         |34845  |7527  |0.49010229839245395|9.787890434265137 |1.193314552307129 |\n",
            "|RDD      |2.0           |8         |34845  |7527  |0.49010229839245395|14.220362901687622|2.5870327949523926|\n",
            "+---------+--------------+----------+-------+------+-------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Grille d'expériences =====================================================\n",
        "\n",
        "fractions = [0.25, 0.5, 1.0, 2.0]   # redimensionne l'ensemble d'entraînement : 25%, 50%, 100%, 200% (200% = échantillonnage avec remise)\n",
        "base_parts = max(train_df.rdd.getNumPartitions(), 2)\n",
        "parts_grid = [base_parts, base_parts * 2, base_parts * 4]   # différents niveaux de parallélisme (nb de partitions / shuffles)\n",
        "\n",
        "rows = []\n",
        "for fr in fractions:\n",
        "    tr = scale_dataset(train_norm, fr)  # redimensionne l'entraînement (<=1 sous-échant., >1 sur-échant.)\n",
        "    for p in parts_grid:\n",
        "        # Régler le parallélisme (nombre de partitions dans les shuffles)\n",
        "        spark.conf.set(\"spark.sql.shuffle.partitions\", p)\n",
        "\n",
        "        # Harmoniser le partitionnement des datasets et matérialiser\n",
        "        tr_p = tr.repartition(p).cache();  tr_p.count()\n",
        "        te_p = test_norm.repartition(p).cache(); te_p.count()\n",
        "\n",
        "        # --- Variante RDD ----\n",
        "        acc, ttrain, tinfer = rdd_run(tr_p, te_p)\n",
        "        rows.append((\"RDD\", fr, p, tr_p.count(), te_p.count(), acc, ttrain, tinfer))\n",
        "\n",
        "        # --- Variante DataFrame ----\n",
        "        acc, ttrain, tinfer = df_run(tr_p, te_p)\n",
        "        rows.append((\"DataFrame\", fr, p, tr_p.count(), te_p.count(), acc, ttrain, tinfer))\n",
        "\n",
        "        # Libérer le cache pour l'itération suivante\n",
        "        tr_p.unpersist(); te_p.unpersist()\n",
        "\n",
        "# Résultats agrégés (un DF pour affichage)\n",
        "schema = [\"approach\",\"train_fraction\",\"partitions\",\"n_train\",\"n_test\",\n",
        "          \"accuracy\",\"train_time_s\",\"infer_time_s\"]\n",
        "res = spark.createDataFrame(rows, schema).orderBy(\"approach\",\"train_fraction\",\"partitions\")\n",
        "n = res.count()\n",
        "res.show(n, truncate=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
